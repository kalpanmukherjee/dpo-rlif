<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DPO RLAIF</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DPO RL with Implicit Feedback</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/amansinghalml/" target="_blank">Aman Singhal</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/kalpanmukherjee/" target="_blank">Kalpan Mukherjee</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">NYU Courant<br>Deep Decision Making and Reinforcement Learning, Spring 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <!-- <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> --> 

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AmanSinghal927/DPO-RL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> --> 
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <figure>
          <img src="static/images/llm_learn_flow.png" alt="Descriptive Text for Image" style="width: 70%; height: auto;">
          <figcaption></figcaption>
        </figure>
        <figure>
          <img src="static/images/preference_data_flow.png" alt="Descriptive Text for Image" style="width: 70%; height: auto;">
          <figcaption></figcaption>
        </figure>
        <div class="content has-text-justified">
          <p>
            The use of Large Language Models (LLMs) in healthcare challenges traditional trust, 
            because generated outputs are untraceable and sometimes inaccurate <a href="https://pubmed.ncbi.nlm.nih.gov/37816837/">[1]</a>. 
            LLMs are trained on conflicting sources of information through
            the maximum likelihood objective in an unsupervised fashion <a href="https://arxiv.org/pdf/2303.18223">[3]</a>. This is
            typically followed by 1) Supervised Fine-Tuning 2) Preference sampling and
            reward learning and 3) Reinforcement Learning optimization (RLHF) <a href="https://arxiv.org/pdf/2005.14165">[4]</a>.
            Prior work such as <a href="https://arxiv.org/pdf/2307.15217">[5]</a> and <a href="https://arxiv.org/pdf/2306.16388">[6]</a> mapped the biases in human annotator preferences 
            in the RLHF stage as the source of trickle-down biases in LLMs.
            Furthermore, RLHF is a complex and often unstable procedure, and obtaining 
            alignment healthcare data at scale is inherently expensive <a href="https://arxiv.org/pdf/2204.05862">[2]</a>. In
            this project, we propose to explore updating the LLM policy network with the 
            family of memory-efficient Direct Preference Optimization(DPO) algorithms <a href="https://arxiv.org/pdf/2305.18290">[7]</a>. 
            To circumvent the need for human annotation we propose to
            leverage Multiple Choice Questions and generate answers for each option
            such that the answer corresponding to the correct option is the de facto chosen response equivalent.
            To benchmark, we propose comparing against the base LLM on the in-distribution 
            test set. Moreover, rigorous testing against datasets like MMLU
            <a href="https://arxiv.org/pdf/2009.03300">[8]</a> splits for anatomy, clinical knowledge, medical genetics, etc. will enable an 
            understanding of generalization trends.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<div style="text-align: center; padding-top: 4%; ">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="padding-top: 2%; padding-bottom: 4%">Direct Preference Optimisation (DPO)</h2> 
      <div class="content has-text-justified">
        <p>
        </p>
      </div>
      <figure>
        <img src="static/images/dpo_table.png" alt="Descriptive Text for Image" style="width: 70%; height: auto;">
        <figcaption></figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          DPO is a method used primarily in preference-based reinforcement learning. In this approach, instead of receiving explicit rewards, the learning algorithm is guided by preferences between different actions provided by an external source. The method is derived from Proximal Policy Optimization (PPO) for Large Language Models by relaxing the need of a critic model. Instead, the loss function is given as:
        </p>
      </div>
      <figure>
        <img src="static/images/dpo_eq_1.png" alt="Descriptive Text for Image" style="width: 70%; height: auto;">
        <figcaption></figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          where, the updated policy’s distribution over preferred and non-preferred responses is explicitly constrained w.r.t the original policy. Here beta corresponds to the strength of the KL-constrain in the original PPO objective given by:
        </p>
      </div>
      <figure>
        <img src="static/images/dpo_eq_2.png" alt="Descriptive Text for Image" style="width: 60%; height: auto;">
        <figcaption></figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          Thus, beta controls the tradeoff-between exploration and exploitation, with higher values of beta enabling higher exploration. 
        </p>
      </div>
    </div>
    </div>
</div>

<div style="text-align: center; padding-top: 4%; ">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="padding-top: 2%; padding-bottom: 4%">Experiment Setup</h2> 
      <div class="content has-text-justified">
        <p>
          We look to further relax the requirement for a preference dataset in two settings, Reinforcement Learning from AI Feedback (RLAIF) and Reinforcement Learning from Implicit Feedback (RLIF). We believe this will utilize existing & abundant resources such as multiple-choice question & answer datasets as well as existing pre-trained models. Our intuition is to lower the cost, speed up the development and remove biases from the training pipeline of LLMs.
        </p>
      </div>
      <figure>
        <img src="static/images/experiment_setup.png" alt="Descriptive Text for Image" style="width: 60%; height: auto;">
        <figcaption></figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          
        </p>
      </div>
    </div>
    </div>
</div>


<div style="text-align: center; padding-top: 4%; ">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="padding-top: 2%; padding-bottom: 4%">RLAIF - Open World Problem</h2> 
      <div class="content has-text-justified">
        <p>
          The <a href="https://huggingface.co/datasets/stanfordnlp/imdb">stanford-nlp IMDB</a> reviews dataset is a collection of movie reviews from IMDB. The dataset contains a total of 50,000 reviews, each of which is annotated with a sentiment label (positive or negative). The dataset is divided into two subsets: the training set and the test set.
          The positive reviews are used for supervised fine-tuning of a base gpt2-large model as a next word prediction task. The model thus learns how to generate positive reviews of movies given a few starting tokens as a prior.
          Now, the first few tokens (2-8) of the entire IMDB reviews dataset (positive and negative) is used prompt the supervised fine-tuned (SFT) gpt2-large model to generate positive reviews. Each prompt generated four reviews, the reviews are varied due to the use of different decoding strategies like temperature settings, beam-search, top-k and top-p.
          Each of these reviews are fed into <a href="https://huggingface.co/siebert/sentiment-roberta-large-english">sentiment-roberta-large</a> model to acertain the positive sentiment score of the reviews. These scores are used to create 6 pairs of reviews from the 4 individual reviews, with the review with the higher score among the two being the "chosen"/"accepted"/"preferred" response
          and the other review being the "rejected"/"unaccepted"/"disliked" response. This, along with the input prompt tokens creates our <b>preference dataset</b>.
         This preference dataset is used to run Direct Preference Optimisation on the SFT gpt2-large model, producing our final <i>aligned</i> gpt2-large model.
        </p>
      </div>
      <figure>
        <img src="static/images/IMDB.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
        <figcaption>Figure 2: RLAIF - Open World Problem</figcaption>
      </figure>
      <figure>
        <img src="static/images/rlaif_reward_hacking.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
        <figcaption>Figure 3: RLAIF - Reward Hacking</figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          As showcased in Figure 3, the Beta value affects the tradeoff between exploration and exploitation. Higher values of beta enable higher exploration, but at the cost of higher exploitation. 
          This is apparent in the eval_ngrams plot where the number of unique ngrams and plotted against training steps for various values of beta. 
          We notice that for beta=0.01, the number of unqiuue ngrams is significantly lower than for higher values of beta.
        </p>
      </div>
      <figure>
        <img src="static/images/reward_hacking_text.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
        <figcaption>Figure 4: Reward Hacking Example</figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <p>
          Comparing the outputs of models trained with two values of beta - 0.01 and 0.6, we can see that the model trained with beta=0.01 repeats some phrases like
          "out of 10" and "miss miss missing miss". Whereas the model with beta=0.6 generates a much more unique and nuanced response.
          It touches on relevant points like "acting", "direction", "soundtrack", "cinematography".
      </div>
      <figure>
        <img src="static/images/sentiment_margins.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
        <figcaption>Figure 5: Reward Margins</figcaption>
      </figure>
    </div>
    </div>
</div>

<div style="text-align: center; padding-top: 4%; ">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="padding-top: 2%; padding-bottom: 4%">RLIF - Closed World Problem</h2> 
      <div class="content has-text-justified">
        <p>
          <figure>
            <img src="static/images/medical_qna.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
            <figcaption>Figure 6: Medical Question Answering</figcaption>
          </figure>
         The <a href="https://huggingface.co/datasets/openlifescienceai/Med-HALT">Medhalt dataset</a> is a multinational dataset derived from medical examinations across various countries and includes multiple choice questions (MCQs) from NEET, AIIMS PG, USMILE, TWMLE etc.
          The dataset contains a total of 16,000 MCQs (Question, Options, Correct Option index). We use a base Llama-2-13-B model to generate "explainations"/"reasons" for why each of the options (whether correct or not) should be the correct choice. Llama-2 is at such a junction
          point in its training and capabilities that it doesn't contradict the prompt (eg. "Actually this option cannot be the correct one") but is still able to hullucinate believable explanations for why the option is correct.
          This data is used to create two datasets. First, the question and options are matched to the explaination for the correct option. This is our SFT dataset. Second, the question and option are matched with a pair of correct option reasoning (remains constant for a question)
           and incorrect option reasoning (the other 3). Thus for each question, we have three rows of such pairs. This creates our <b>preference dataset</b>
          with the correct option reasoning as the "chosen"/"accepted"/"preferred" response and the other three reasonings as the "rejected"/"unaccepted"/"disliked" response.
          Like before, we use the SFT dataset to train the base gpt2-large model and then use the preference dataset to run Direct Preference Optimisation on the SFT gpt2-large model.
        </p>
      </div>
      <figure>
        <img src="static/images/Medhalt.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
        <figcaption>Figure 7: RLIF - Closed World Problem</figcaption>
      </figure>
      <div class="content has-text-justified" style="padding-top: 4%;">
        <figure>
          <img src="static/images/performance_medical.png" alt="Descriptive Text for Image" style="width: 80%; height: auto;">
          <figcaption>Figure 8: RLIF - MMLU Accuracy</figcaption>
        </figure>
        <p>
          [TODO] Dicussion on result...
          [TODO] Shocking result: OOD generalisation to MMLU is same as if a GPT-2 model was actually fine-tuned on MMLU
          [TODO] Chart (Art 4): baseline GPT-2 as good as random, finetuned on non-MMLU data (30%), DPO (x%)
          Eval methodology: max(log probs on MCQ choices|question) == correct answer choice
          Zero out the log probs for the question/prompt
          
        </p>
      </div>
    </div>
    </div>
</div>


</body>


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
       Your image here -->
        <!-- <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        Your image here -->
        <!-- <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> --> 
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
       Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>  -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
             Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
             Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>  -->
<!-- End video carousel -->







<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->



<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
